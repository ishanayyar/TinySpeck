Main areas to look at to optimize - since we have a resource-constrained environment:


Minutes from 11th Feb

Hyperparameter
- figure with 36 million parameters
- figure with muchhhh fewer parameters —> similar output
- graphics card -> what model 4060?

— on windows, save as .keras

- for handheld device, maybe 256 is enough
- not changing data, just filter

- compress data?
- starting filter? Can we train the data on a few models to decide how many filters we need?
- what would be the best at the start? Train on 8, 32, 64, 128
—> figure of merit

- still 200 epochs; callback —> save the best model
- when after 10 epochs, no improvement  —>> another callback

- change number of encoder-decoder blocks
12 networks —> 4 diff number of encoder-decoder blocks, and diff filters
- save learning rate/training curve and validation loss —> save best model


- max pooling: increase number of channels by factor of 2; decrease feature map size by factor of 4: consider stride of 2, 3, and 4 
- reduce tensor 


SO TRAINING 36 models in total



####################
Optimize input/output resolution as we discussed
####################
- we can downsample inputs 
- Raman has lots of redundant data --> downsampling to 512 points instead of 1024+ --> retain sufficient detail; reduce computational load
- effect on Raman Shift --> downsampling shouldn't affect meaningful spectral peaks

Scaling output res: does current output res match the necessary precision??


https://www.edinst.com/resource/spectral-resolution-in-raman-spectroscopy/
https://www.nature.com/articles/s41377-024-01394-5
https://static.horiba.com/fileadmin/Horiba/Products/Scientific/Molecular_and_Microanalysis/Raman_General/Spectral_Resolution_Tech_Note_RA-TN14.pdf
--> very interesting to see CCD v spectral res

https://www.photonics.com/Articles/Selecting_CCDs_for_Raman_Spectroscopy/a45915

--> specifically mentions handheld Raman devices



###################
Fewer layers/filters
###################
- might need to reduce filters systematically (eg. halve filter count per layer) + check for performance loss
- fewer epochs: training for only 50 epochs -> sped up training but maybe affecting model convergence. EARLY STOPPING?
- shallower network: can network perform with fewer encoder-decoder blocks????
- might need to prune away some weights

###################
Data cleaning
###################
- noise handling: additional preprocessing?
- raw data compatibility: running on Raspberry Pi; could preprocess data separately to avoid runtime processing overhead?

####################
Quantization /Discretization
####################
Raspberry Pi --> fine to handle int8, so no need to discretize --> test w/ TFLite --> not needed for training, but good for inference
ARM Chips (cheaper than raspberry pi)
- lower precision -from 32-bit fp to 8-bit int (post-training quantization) to reduce model size
- TensorFlow Lite(TFLite): can we test TFLite version w/quantization -int8?

####################
Deployment
####################

- benchmark performance on Raspberry Pi Zero and Pi 5 --> very different performance levels
- need model to fit within available RAM w/o excessive swapping
- depthwise separable convolutions? More efficient?

